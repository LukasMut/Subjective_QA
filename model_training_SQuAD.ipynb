{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T18:23:22.139208Z",
     "start_time": "2020-02-04T18:23:17.353072Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "import keras\n",
    "import os\n",
    "import re\n",
    "import torch \n",
    "import transformers\n",
    "import gzip\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import islice\n",
    "from tqdm import trange, tqdm\n",
    "from torch.optim import Adam\n",
    "from transformers import BertTokenizer, BertModel, BertForQuestionAnswering\n",
    "from transformers import AdamW\n",
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from models.QAheads import *\n",
    "from models.utils import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T18:23:23.305482Z",
     "start_time": "2020-02-04T18:23:23.302485Z"
    }
   },
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T18:13:14.423143Z",
     "start_time": "2020-02-04T18:13:12.780Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# load SubjQA_data into memory\n",
    "subjqa_data_train = get_data(source='/SubjQA/', split='/train', domain='all')\n",
    "subjqa_data_dev = get_data(source='/SubjQA/', split='/dev', domain='all')\n",
    "subjqa_data_test = get_data(source='/SubjQA/', split='/test', domain='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T18:23:26.615906Z",
     "start_time": "2020-02-04T18:23:25.994485Z"
    }
   },
   "outputs": [],
   "source": [
    "# load SQuAD_data into memory\n",
    "squad_data_train = get_data(\n",
    "                            source='/SQuAD/',\n",
    "                            split='train',\n",
    "                            )\n",
    "\n",
    "#squad_data_dev = get_data(\n",
    "#                          source='/SQuAD/',\n",
    "#                          split='dev',\n",
    "#                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and dev QA examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-04T18:23:27.767Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: figure out, whether we should use pretrained weights from 'bert-base-cased' or 'bert-base-uncased' model\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# BERT cannot deal with sequences, where T > 512\n",
    "max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-04T18:23:29.327Z"
    }
   },
   "outputs": [],
   "source": [
    "squad_examples_train = create_examples(\n",
    "                                       squad_data_train,\n",
    "                                       is_training=True,\n",
    "                                       )\n",
    "\n",
    "# create train and dev examples from train set only\n",
    "squad_examples_train, squad_examples_dev = split_into_train_and_dev(squad_examples_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-04T18:23:30.709Z"
    }
   },
   "outputs": [],
   "source": [
    "squad_features_train = convert_examples_to_features(\n",
    "                                                    squad_examples_train, \n",
    "                                                    bert_tokenizer,\n",
    "                                                    max_seq_length=max_seq_length,\n",
    "                                                    doc_stride=100,\n",
    "                                                    max_query_length=50,\n",
    "                                                    is_training=True,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-04T18:23:32.126Z"
    }
   },
   "outputs": [],
   "source": [
    "squad_features_dev = convert_examples_to_features(\n",
    "                                                squad_examples_dev, \n",
    "                                                bert_tokenizer,\n",
    "                                                max_seq_length=max_seq_length,\n",
    "                                                doc_stride=100,\n",
    "                                                max_query_length=50,\n",
    "                                                is_training=True,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-04T18:23:32.441Z"
    }
   },
   "outputs": [],
   "source": [
    "squad_tensor_dataset_train = create_tensor_dataset(\n",
    "                                                   squad_features_train,\n",
    "                                                   evaluate=False,\n",
    "                                                   )\n",
    "\n",
    "squad_tensor_dataset_dev = create_tensor_dataset(\n",
    "                                                 squad_features_dev,\n",
    "                                                 evaluate=False,\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and dev dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-04T18:23:33.763Z"
    }
   },
   "outputs": [],
   "source": [
    "squad_train_dl = create_batches(\n",
    "                                dataset=squad_tensor_dataset_train,\n",
    "                                batch_size=8,\n",
    "                                split='train',\n",
    "                                )\n",
    "\n",
    "squad_dev_dl = create_batches(\n",
    "                              dataset=squad_tensor_dataset_dev,\n",
    "                              batch_size=8,\n",
    "                              split='eval',\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-04T18:23:34.778Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bert_encoder = BertModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "bert_encoder = BertModel.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# initialise QA heads\n",
    "linear_head = LinearQAHead()\n",
    "recurrent_head = RecurrentQAHead(max_seq_length=max_seq_length)\n",
    "\n",
    "# cast models to device\n",
    "bert_encoder.to(device)\n",
    "linear_head.to(device)\n",
    "recurrent_head.to(device)\n",
    "\n",
    "# set current QA head\n",
    "qa_head = recurrent_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T18:17:45.755058Z",
     "start_time": "2020-02-04T18:17:45.752034Z"
    }
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "        \"n_epochs\": 3,\n",
    "        \"lr\": 1e-3,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"max_grad_norm\": 10,\n",
    "        \"squad\": True,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T18:19:58.284380Z",
     "start_time": "2020-02-04T18:19:20.766366Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ BERT model is set to evaluation mode. -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Training of QA head. BERT model is frozen. ------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|                                                                              | 0/1909 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EncoderLSTM' object has no attribute 'bidir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d29dd1bbd6db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m      \u001b[0msquad_train_dl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m      \u001b[0msquad_dev_dl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m      \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     )\n",
      "\u001b[1;32m<ipython-input-13-6af9af815518>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(bert_encoder, qa_head, train_dl, val_dl, args)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqa_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lstm_encoder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                 \u001b[0mstart_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqa_head\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_reps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_input_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mstart_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqa_head\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_reps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Subjective_QA\\models\\QAheads.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, bert_outputs, seq_lengths, start_positions, end_positions)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mhidden_lstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m# pass bert hidden representations through Bi-LSTM to compute temporal dependencies (and global interactions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Subjective_QA\\models\\Encoder.py\u001b[0m in \u001b[0;36minit_hidden\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     55\u001b[0m                     ):\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m# NOTE: we need to initialise twice as many hidden states for bidirectional RNNs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbidir\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mcell_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    583\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 585\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EncoderLSTM' object has no attribute 'bidir'"
     ]
    }
   ],
   "source": [
    "train(\n",
    "     bert_encoder,\n",
    "     qa_head,\n",
    "     squad_train_dl,\n",
    "     squad_dev_dl,\n",
    "     args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T18:18:16.870286Z",
     "start_time": "2020-02-04T18:18:16.850332Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "          bert_encoder, \n",
    "          qa_head,\n",
    "          train_dl,\n",
    "          val_dl,\n",
    "          args,\n",
    "          ):\n",
    "    \n",
    "    t_total = len(train_dl) * args['n_epochs'] # total number of training steps (i.e., step = iteration)\n",
    "\n",
    "    # set pre-trained BERT model to eval mode, if we want to evaluate model's performance on SQuAD\n",
    "    if args[\"squad\"]:\n",
    "        bert_encoder.eval()\n",
    "        print(\"------ BERT model is set to evaluation mode. -------\")\n",
    "        \n",
    "    else:        \n",
    "        bert_optimizer = AdamW(\n",
    "                          bert_encoder.parameters(), \n",
    "                          lr=args['lr'], \n",
    "                          correct_bias=False,\n",
    "                          )\n",
    "    \n",
    "        bert_scheduler = get_linear_schedule_with_warmup(\n",
    "                                                        bert_optimizer, \n",
    "                                                        num_warmup_steps=args[\"warmup_steps\"], \n",
    "                                                        num_training_steps=t_total,\n",
    "                                                        )\n",
    "    \n",
    "    # store loss and accuracy for plotting\n",
    "    train_loss = []\n",
    "    train_accs = []\n",
    "    train_f1s = []\n",
    "    val_accs = []\n",
    "    val_f1s = []\n",
    "    \n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "        \n",
    "    qa_head_optimizer = Adam(\n",
    "                            qa_head.parameters(), \n",
    "                            lr=args['lr'],\n",
    "                            )\n",
    "\n",
    "\n",
    "    for _ in trange(args['n_epochs'],  desc=\"Epoch\"):\n",
    "\n",
    "        ### Training ###\n",
    "        \n",
    "        # set model to train mode (as opposed to eval mode)\n",
    "        if not args[\"squad\"]:\n",
    "            print(\"------ Fine-tuning pre-trained BERT model AND training of QA head. ------\")\n",
    "            bert_encoder.train()\n",
    "            \n",
    "        else:\n",
    "            print(\"------ Training of QA head. BERT model is frozen. ------\")\n",
    "\n",
    "        # we need to train the QA head for both SQuAD and SubjQA\n",
    "        qa_head.train()\n",
    "\n",
    "        tr_loss, tr_acc = 0, 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        \n",
    "        total_loss = 0\n",
    "\n",
    "\n",
    "        for step, batch in enumerate(tqdm(train_dl, desc=\"Iteration\")):\n",
    "\n",
    "            # add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # unpack inputs from dataloader            \n",
    "            b_input_ids, b_attn_masks, b_token_type_ids, b_input_lengths, b_start_pos, b_end_pos, b_cls_indexes, _ = batch\n",
    "            \n",
    "            if args[\"squad\"]:\n",
    "                # zero-out gradients\n",
    "                qa_head_optimizer.zero_grad()\n",
    "            else:\n",
    "                bert_optimizer.zero_grad()\n",
    "                bert_scheduler.zero_grad()\n",
    "                qa_head_optimizer.zero_grad()\n",
    "            \n",
    "            bert_reps = bert_encoder(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_attn_masks)\n",
    "            \n",
    "            if hasattr(qa_head, 'lstm_encoder'):\n",
    "                start_logits, end_logits = qa_head(bert_reps, b_input_lengths)\n",
    "            else:\n",
    "                start_logits, end_logits = qa_head(bert_reps)\n",
    "            \n",
    "            # start and end loss need to be computed separately\n",
    "            \n",
    "            start_loss = loss_func(start_logits, b_start_pos)\n",
    "            end_loss = loss_func(end_logits, b_end_pos)\n",
    "            \n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "            print(\"Current loss: {}\".format(total_loss))\n",
    "\n",
    "            train_loss.append(total_loss.item())\n",
    "                \n",
    "            #start_logits = _to_cpu(start_logits)\n",
    "            \n",
    "            #log_probas = F.log_softmax(logits, dim=1).numpy() # or F.softmax(logits).numpy()\n",
    "            #preds_flat = np.argmax(log_probas, axis=1).flatten()\n",
    "\n",
    "\n",
    "            # backpropagation\n",
    "            #start_loss.backward()\n",
    "            #end_loss.backward()\n",
    "            total_loss.backward()\n",
    "            \n",
    "            if args[\"squad\"]:\n",
    "                torch.nn.utils.clip_grad_norm_(qa_head.parameters(), args[\"max_grad_norm\"])\n",
    "                \n",
    "                # update qa_head parameters and take a step using the computed gradient\n",
    "                qa_head_optimizer.step()\n",
    "\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(bert_encoder.parameters(), args[\"max_grad_norm\"])\n",
    "                torch.nn.utils.clip_grad_norm_(qa_head.parameters(), args[\"max_grad_norm\"])\n",
    "\n",
    "                # update model parameters and take a step using the computed gradient\n",
    "                bert_optimizer.step()\n",
    "                bert_scheduler.step()\n",
    "                qa_head_optimizer.step()\n",
    "\n",
    "            tr_loss += total_loss.item()\n",
    "            \n",
    "            #tr_acc += tr_acc_current\n",
    "            #tr_f1 += tr_f1_current\n",
    "\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "        break\n",
    "        \n",
    "        #print(\"Train acc: {}\".format(tr_acc/nb_tr_steps))\n",
    "        #print(\"Train f1: {}\".format(tr_f1/nb_tr_steps))\n",
    "        \n",
    "        #train_accs.append(tr_acc/nb_tr_steps)\n",
    "        #train_f1s.append(tr_f1/nb_tr_steps)\n",
    "\n",
    "        \n",
    "\n",
    "        ### Validation ###\n",
    "\n",
    "        # set models to eval mode to evaluate loss on the validation set\n",
    "        if not args[\"squad\"]:\n",
    "            bert_encoder.eval()\n",
    "            \n",
    "        qa_head.eval()\n",
    "\n",
    "        eval_acc, eval_f1 = 0, 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        for batch in val_dl:\n",
    "\n",
    "            # add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # unpack inputs from dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            # telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if model_name == 'DistilBERT':\n",
    "                    logits = model(b_input_ids, attention_mask=b_input_mask)\n",
    "                else:\n",
    "                    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "            # move logits and labels to CPU to compute NumPy operations\n",
    "            logits = logits[0].detach().cpu()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            \n",
    "            log_probas = F.log_softmax(logits, dim=1).numpy() # or F.softmax(logits).numpy()\n",
    "            preds_flat = np.argmax(log_probas, axis=1).flatten()\n",
    "\n",
    "            tmp_eval_acc = accuracy(preds_flat, label_ids)\n",
    "            tmp_eval_f1 = f1(preds_flat, label_ids)\n",
    "\n",
    "            eval_acc += tmp_eval_acc\n",
    "            eval_f1 += tmp_eval_f1\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        print(\"Val acc: {}\".format(eval_acc/nb_eval_steps))\n",
    "        print(\"Val f1: {}\".format(eval_f1/nb_eval_steps))\n",
    "        val_accs.append(eval_acc/nb_eval_steps)\n",
    "        val_f1s.append(eval_f1/nb_eval_steps)\n",
    "        \n",
    "    return train_loss, train_accs, train_f1s, val_accs, val_f1s, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T14:26:02.646693Z",
     "start_time": "2020-02-03T14:26:02.631069Z"
    }
   },
   "outputs": [],
   "source": [
    "support, question = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T14:26:06.189222Z",
     "start_time": "2020-02-03T14:26:06.185213Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(support, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T11:31:59.279076Z",
     "start_time": "2020-02-03T11:31:59.274088Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_dict = tokenizer.encode_plus(support, question, max_length=19, pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T11:32:21.237260Z",
     "start_time": "2020-02-03T11:32:21.233246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_dict['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T14:26:11.063555Z",
     "start_time": "2020-02-03T14:26:11.053853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] who was jim henson? [SEP] jim henson was a nice puppet [SEP]'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T14:26:17.041808Z",
     "start_time": "2020-02-03T14:26:16.871864Z"
    }
   },
   "outputs": [],
   "source": [
    "token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n",
    "bert_outputs = bert_encoder(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n",
    "start_scores, end_scores = linear_head(bert_outputs)\n",
    "\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n",
    "\n",
    "#assert answer == \"a nice puppet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T14:26:19.955818Z",
     "start_time": "2020-02-03T14:26:19.933131Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'who',\n",
       " 'was',\n",
       " 'jim',\n",
       " 'henson',\n",
       " '?',\n",
       " '[SEP]',\n",
       " 'jim',\n",
       " 'henson',\n",
       " 'was',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'puppet',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T14:26:53.048321Z",
     "start_time": "2020-02-03T14:26:53.043301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3921,  0.7497,  0.8149,  0.7072,  0.1412,  0.7454,  0.3920,  0.6636,\n",
       "         0.1638,  0.5663,  0.5980, -0.2040, -0.1426,  0.3924],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T14:26:27.278089Z",
     "start_time": "2020-02-03T14:26:27.274099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
